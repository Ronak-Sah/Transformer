{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc878a8-706a-4d9c-9878-019438ed954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpetokenizer import BPETokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d33b7b1-779e-4c53-9009-abc73ec39e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging 1/54: (32, 116) -> 256 (b' t') had 7 frequency\n",
      "merging 2/54: (101, 120) -> 257 (b'ex') had 3 frequency\n",
      "merging 3/54: (257, 116) -> 258 (b'ext') had 3 frequency\n",
      "merging 4/54: (256, 111) -> 259 (b' to') had 3 frequency\n",
      "merging 5/54: (101, 110) -> 260 (b'en') had 3 frequency\n",
      "merging 6/54: (60, 124) -> 261 (b'<|') had 2 frequency\n",
      "merging 7/54: (115, 116) -> 262 (b'st') had 2 frequency\n",
      "merging 8/54: (111, 102) -> 263 (b'of') had 2 frequency\n",
      "merging 9/54: (263, 116) -> 264 (b'oft') had 2 frequency\n",
      "merging 10/54: (264, 258) -> 265 (b'oftext') had 2 frequency\n",
      "merging 11/54: (124, 62) -> 266 (b'|>') had 2 frequency\n",
      "merging 12/54: (105, 115) -> 267 (b'is') had 2 frequency\n",
      "merging 13/54: (32, 97) -> 268 (b' a') had 2 frequency\n",
      "merging 14/54: (32, 115) -> 269 (b' s') had 2 frequency\n",
      "merging 15/54: (256, 104) -> 270 (b' th') had 2 frequency\n",
      "merging 16/54: (270, 101) -> 271 (b' the') had 2 frequency\n",
      "merging 17/54: (259, 107) -> 272 (b' tok') had 2 frequency\n",
      "merging 18/54: (272, 260) -> 273 (b' token') had 2 frequency\n",
      "merging 19/54: (32, 91) -> 274 (b' [') had 2 frequency\n",
      "merging 20/54: (83, 80) -> 275 (b'SP') had 2 frequency\n",
      "merging 21/54: (275, 69) -> 276 (b'SPE') had 2 frequency\n",
      "merging 22/54: (276, 67) -> 277 (b'SPEC') had 2 frequency\n",
      "merging 23/54: (277, 73) -> 278 (b'SPECI') had 2 frequency\n",
      "merging 24/54: (278, 65) -> 279 (b'SPECIA') had 2 frequency\n",
      "merging 25/54: (279, 76) -> 280 (b'SPECIAL') had 2 frequency\n",
      "merging 26/54: (262, 97) -> 281 (b'sta') had 1 frequency\n",
      "merging 27/54: (281, 114) -> 282 (b'star') had 1 frequency\n",
      "merging 28/54: (282, 116) -> 283 (b'start') had 1 frequency\n",
      "merging 29/54: (283, 265) -> 284 (b'startoftext') had 1 frequency\n",
      "merging 30/54: (32, 72) -> 285 (b' H') had 1 frequency\n",
      "merging 31/54: (285, 101) -> 286 (b' He') had 1 frequency\n",
      "merging 32/54: (286, 108) -> 287 (b' Hel') had 1 frequency\n",
      "merging 33/54: (287, 108) -> 288 (b' Hell') had 1 frequency\n",
      "merging 34/54: (288, 111) -> 289 (b' Hello') had 1 frequency\n",
      "merging 35/54: (32, 87) -> 290 (b' W') had 1 frequency\n",
      "merging 36/54: (290, 111) -> 291 (b' Wo') had 1 frequency\n",
      "merging 37/54: (291, 114) -> 292 (b' Wor') had 1 frequency\n",
      "merging 38/54: (292, 108) -> 293 (b' Worl') had 1 frequency\n",
      "merging 39/54: (293, 100) -> 294 (b' World') had 1 frequency\n",
      "merging 40/54: (32, 84) -> 295 (b' T') had 1 frequency\n",
      "merging 41/54: (295, 104) -> 296 (b' Th') had 1 frequency\n",
      "merging 42/54: (296, 267) -> 297 (b' This') had 1 frequency\n",
      "merging 43/54: (32, 267) -> 298 (b' is') had 1 frequency\n",
      "merging 44/54: (269, 97) -> 299 (b' sa') had 1 frequency\n",
      "merging 45/54: (299, 109) -> 300 (b' sam') had 1 frequency\n",
      "merging 46/54: (300, 112) -> 301 (b' samp') had 1 frequency\n",
      "merging 47/54: (301, 108) -> 302 (b' sampl') had 1 frequency\n",
      "merging 48/54: (302, 101) -> 303 (b' sample') had 1 frequency\n",
      "merging 49/54: (256, 258) -> 304 (b' text') had 1 frequency\n",
      "merging 50/54: (32, 119) -> 305 (b' w') had 1 frequency\n",
      "merging 51/54: (305, 105) -> 306 (b' wi') had 1 frequency\n",
      "merging 52/54: (306, 116) -> 307 (b' wit') had 1 frequency\n",
      "merging 53/54: (307, 104) -> 308 (b' with') had 1 frequency\n",
      "merging 54/54: (269, 112) -> 309 (b' sp') had 1 frequency\n",
      "Total time taken: 0.00 seconds\n",
      "Throughput: 16976.69 chunks/second\n"
     ]
    }
   ],
   "source": [
    "special_tokens = {\n",
    "    \"<|endoftext|>\": 1001,\n",
    "    \"<|startoftext|>\": 1002,\n",
    "    \"[SPECIAL1]\": 1003,\n",
    "    \"[SPECIAL2]\": 1004,\n",
    "}\n",
    "\n",
    "tokenizer = BPETokenizer(special_tokens=special_tokens)\n",
    "texts = \"<|startoftext|> Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.<|endoftext|>\"\n",
    "\n",
    "tokenizer.train(texts, vocab_size=310, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0210694-0629-4123-8f7c-9312b5b7360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 261, 284, 266, 72, 101, 108, 108, 111, 44, 294, 33, 297, 298, 268, 303, 304, 308, 271, 309, 101, 99, 105, 97, 108, 273, 115, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 259, 256, 101, 262, 271, 273, 105, 122, 101, 114, 46, 10, 72, 101, 108, 108, 111, 44, 32, 85, 110, 105, 118, 101, 114, 115, 101, 33, 32, 65, 110, 111, 116, 104, 101, 114, 32, 257, 97, 109, 112, 108, 101, 269, 260, 116, 260, 99, 101, 32, 99, 111, 110, 116, 97, 105, 110, 105, 110, 103, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 44, 32, 117, 115, 101, 100, 259, 32, 260, 115, 117, 114, 101, 273, 105, 122, 101, 114, 39, 115, 32, 114, 111, 98, 117, 262, 110, 101, 115, 115, 46, 10, 71, 114, 101, 101, 116, 105, 110, 103, 115, 44, 32, 69, 97, 114, 116, 104, 33, 286, 114, 101, 305, 101, 32, 104, 97, 118, 101, 274, 280, 49, 93, 268, 112, 112, 101, 97, 114, 105, 110, 103, 32, 111, 110, 99, 101, 268, 103, 97, 105, 110, 44, 32, 102, 111, 108, 108, 111, 119, 101, 100, 32, 98, 121, 274, 280, 50, 93, 32, 105, 110, 271, 300, 101, 269, 260, 116, 260, 99, 101, 46, 10, 72, 101, 108, 108, 111, 44, 294, 33, 297, 298, 32, 121, 101, 116, 268, 110, 111, 116, 104, 101, 114, 303, 304, 44, 308, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 109, 97, 107, 105, 110, 103, 268, 110, 268, 112, 112, 101, 97, 114, 97, 110, 99, 101, 46, 10, 72, 101, 121, 271, 114, 101, 44, 294, 33, 295, 101, 262, 105, 110, 103, 271, 273, 105, 122, 101, 114, 308, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 259, 269, 101, 101, 32, 105, 102, 32, 105, 116, 32, 104, 97, 110, 100, 108, 101, 115, 309, 101, 99, 105, 97, 108, 273, 115, 32, 112, 114, 111, 112, 101, 114, 108, 121, 46, 10, 83, 97, 108, 117, 116, 97, 116, 105, 111, 110, 115, 44, 32, 80, 108, 97, 110, 101, 116, 33, 296, 101, 273, 105, 122, 101, 114, 269, 104, 111, 117, 108, 100, 32, 114, 101, 99, 111, 103, 110, 105, 122, 101, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 105, 110, 270, 267, 32, 108, 111, 110, 103, 32, 262, 114, 105, 110, 103, 32, 263, 304, 46, 10, 72, 101, 108, 108, 111, 268, 103, 97, 105, 110, 44, 294, 33, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 268, 114, 101, 309, 101, 99, 105, 97, 108, 273, 115, 270, 97, 116, 32, 110, 101, 101, 100, 259, 32, 98, 101, 32, 104, 97, 110, 100, 108, 101, 100, 32, 99, 111, 114, 114, 101, 99, 116, 108, 121, 32, 98, 121, 271, 273, 105, 122, 101, 114, 46, 10, 87, 101, 108, 99, 111, 109, 101, 44, 294, 33, 32, 73, 110, 99, 108, 117, 100, 105, 110, 103, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 109, 117, 108, 116, 105, 112, 108, 101, 256, 105, 109, 101, 115, 32, 105, 110, 270, 267, 32, 108, 97, 114, 103, 101, 304, 259, 32, 260, 115, 117, 114, 101, 32, 112, 114, 111, 112, 101, 114, 32, 260, 99, 111, 100, 105, 110, 103, 46, 10, 72, 105, 44, 294, 33, 32, 76, 101, 116, 39, 115, 268, 100, 100, 274, 280, 49, 93, 268, 110, 100, 274, 280, 50, 93, 32, 105, 110, 32, 118, 97, 114, 105, 111, 117, 115, 32, 112, 97, 114, 116, 115, 32, 263, 270, 267, 32, 108, 111, 110, 103, 269, 260, 116, 260, 99, 101, 259, 256, 101, 262, 271, 273, 105, 122, 101, 114, 270, 111, 114, 111, 117, 103, 104, 108, 121, 46, 10, 261, 260, 100, 265, 266, 10]\n",
      "\n",
      "<|startoftext|>Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.\n",
      "Hello, Universe! Another example sentence containing [SPECIAL1] and [SPECIAL2], used to ensure tokenizer's robustness.\n",
      "Greetings, Earth! Here we have [SPECIAL1] appearing once again, followed by [SPECIAL2] in the same sentence.\n",
      "Hello, World! This is yet another sample text, with [SPECIAL1] and [SPECIAL2] making an appearance.\n",
      "Hey there, World! Testing the tokenizer with [SPECIAL1] and [SPECIAL2] to see if it handles special tokens properly.\n",
      "Salutations, Planet! The tokenizer should recognize [SPECIAL1] and [SPECIAL2] in this long string of text.\n",
      "Hello again, World! [SPECIAL1] and [SPECIAL2] are special tokens that need to be handled correctly by the tokenizer.\n",
      "Welcome, World! Including [SPECIAL1] and [SPECIAL2] multiple times in this large text to ensure proper encoding.\n",
      "Hi, World! Let's add [SPECIAL1] and [SPECIAL2] in various parts of this long sentence to test the tokenizer thoroughly.\n",
      "<|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encode_text = \"\"\"\n",
    "<|startoftext|>Hello, World! This is a sample text with the special tokens [SPECIAL1] and [SPECIAL2] to test the tokenizer.\n",
    "Hello, Universe! Another example sentence containing [SPECIAL1] and [SPECIAL2], used to ensure tokenizer's robustness.\n",
    "Greetings, Earth! Here we have [SPECIAL1] appearing once again, followed by [SPECIAL2] in the same sentence.\n",
    "Hello, World! This is yet another sample text, with [SPECIAL1] and [SPECIAL2] making an appearance.\n",
    "Hey there, World! Testing the tokenizer with [SPECIAL1] and [SPECIAL2] to see if it handles special tokens properly.\n",
    "Salutations, Planet! The tokenizer should recognize [SPECIAL1] and [SPECIAL2] in this long string of text.\n",
    "Hello again, World! [SPECIAL1] and [SPECIAL2] are special tokens that need to be handled correctly by the tokenizer.\n",
    "Welcome, World! Including [SPECIAL1] and [SPECIAL2] multiple times in this large text to ensure proper encoding.\n",
    "Hi, World! Let's add [SPECIAL1] and [SPECIAL2] in various parts of this long sentence to test the tokenizer thoroughly.\n",
    "<|endoftext|>\n",
    "\"\"\"\n",
    "ids = tokenizer.encode(encode_text, special_tokens=\"all\")\n",
    "print(ids)\n",
    "\n",
    "decode_text = tokenizer.decode(ids)\n",
    "print(decode_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow GPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
