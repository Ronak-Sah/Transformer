model_trainer:
  epochs: 10
  emb_dim: 128
  ffn_hidden: 514
  num_heads: 4
  drop_prob: 0.1
  num_layers: 6
  max_sequence_length : 800
  batch_size : 12